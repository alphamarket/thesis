\قسمت{روش‌های انتخاب عمل}
همان‌طور که در یادگیری تقویتی گفته شد یکی از وظایف عاملی که از یادگیری تقویتی استفاده می‌کند انتخاب عمل در طول یادگیری است.عامل بعد از رسیدن به هر حالت تا زمان رسیدن به یک حالت پایانی باید اعمالی انتخاب نماید. این انتخاب می‌تواند کاملاً بر اساس داده‌های جمع‌آوری شده در چرخه‌های یادگیری قبلی باشد که اصطلاحاً بهره‌برداری نامیده میشود. برای این کار کافی است در هر حرکت عملی انتخاب شود که در جدول Q ارزش بالاتری دارد اما این کار باعث میشود تا عامل مسیرهایی را تکرار نماید و شرط بررسی تمام‌مسیرها که از فرایند تصادفی مارکو به یادگیری تقویتی رسیده است را ارضا نمی‌کند. پس عامل نیاز است در طول یادگیری گاهی فارغ از بهترین عمل اعمال دیگر را نیز بررسی نماید. که این بررسی اعمال دیگر را اکتشاف گویند.در بدترین حالت می‌توان گفت که در طول یادگیری عامل به‌صورت تصادفی اعمال را انتخاب نماید اما این موضوع فرایند یادگیری را بسیار طولانی می‌کند. پس عامل باید برای رسیدن به یک تعادل در اکتشاف و بهره‌برداری یک روش مناسب در انتخاب اعمال داشته باشد.در ادامه دو روش انتخاب عمل که در یادگیری مشارکتی مورداستفاده قرار می‌گیرد خواهد آمد.

\زیرقسمت{$\varepsilon$-حریصانه}
در $\varepsilon$-حریصانه جهت رسیدن به یک تعادل در اکتشاف و بهره‌برداری یک پارامتر $\varepsilon$ که مقداری بین 0 و 1 است در نظر گرفته میشود. سپس به‌احتمال $\varepsilon$ اکتشاف و به‌احتمال
($1 - \varepsilon$)
بهره‌برداری از داده‌های جدول $Q$ انجام می‌شود. جهت پیاده‌سازی این روش یک مقدار تصادفی  $\xi$ بین 0 و 1 تولیدشده و مطابق رابطه زیر درصورتی‌که این $\xi$ کوچک‌تر از $\varepsilon$ باشد حرکت تصادفی و در غیر این صورت بهترین حرکت بر اساس داده‌های جدول $Q$ انجام میشود.

\زیرقسمت{بولتزمن}
روش $\varepsilon$-حریصانه توانسته تا حدودی بین اکتشاف و بهره‌برداری تعادل ایجاد نماید اما احتمال انتخاب اعمال در $\varepsilon$-حریصانه ثابت بوده و ارتباطی باارزش اعمال در جداول $Q$ ندارد. روش بولتزمن با بهره‌گیری از رابطه \ref{eq:boltzmann_softmax} سعی دارد تا احتمال انتخاب عمل $i$ام را در یک موقعیت $s$ بر اساس ارزش اعمال در همان موقعیت در جدول $Q$ محاسبه شود. همچنین در این رابطه مشخص است این روش دارای پارامتری به‌عنوان $\tau$ برای کنترل حساست به اختلاف ارزش بین اعمال در نظر گرفته‌شده است هرچقدر این میزان بزرگ‌تر باشد به اختلاف ارزش‌ها اهمیت کمتری می‌شود. یعنی در صورتی که $\tau \rightarrow \infty$ تمامی اعمال ممکن در موقعیت $s$ به احتمال یکسانی انتخاب می‌شود و در صورتی که $\tau \rightarrow 0$ به صورت حریصانه عملی انتخاب می‌شود که مقدار $Q$ بیشتری دارد.
\begin{equation}\label{eq:boltzmann_softmax}
p_{Q_s}(i) = {e^{Q_s(i) \over \tau} \over \sum\limits_{j = 1}^n e^{Q_s(j) \over \tau}}
\end{equation}