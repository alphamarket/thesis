\قسمت{یادگیری تقویتی}
یادگیری تقویتی معمولا به آن سری از روش‌های یادگیری گفته میشود که عامل به دنبال رسیدن به یادگیری از طرق ارتباط با محیط است. در این دسته از روش‌ها به عامل ارزش اعمال گفته نمی‌شود و عامل با تعاملی که با محیط دارد باید ارزش اعمال را کشف نماید. در این روش فرایند یادگیری به بخش‌هایی با عنوان چرخه یادگیری شکسته می‌شود، که الگوریتم \ref{alg:q-learning} به ازای هر چرخه‌ی یادگیری به اجرا در می‌آید. هر چرخه یادگیری از قرار دادن عامل در یک حالت تصادفی شروع و تا رسیدن به یک حالت پایانی ادامه دارد. در طول هر چرخه تا زمان رسیدن به حالت پایانی عامل وظیفه دارد عمل را انتخاب نماید و پس از دریافت پاداش عمل انجام‌شده به بروز رسانی اطلاعات  بپردازد. بروز رسانی داده‌ها بر اساس فرایند تصادفی مارکف و روش برنامه نویسی پویا می‌تواند انجام شود. بر اساس تئوری ارائه‌شده در فرایند تصادفی مارکف باید انتخاب عمل به صورتی باشد که انجام هر عمل در هر حالت ضمانت شود. معمولا در یادگیری تقویتی این انتخاب عمل به‌وسیله روش‌هایی چون بولتزمن انجام میشود. در شکل \ref{fig:rl_schema} می‌توان فرایند یادگیری را مشاهده کرد.

\begin{algorithm}[t]\setstretch{1.2}
\caption{\rl{الگوریتم یادگیری $Q$\مرجع{poole2010artificial}}}\label{alg:q-learning}
\begin{latin}
\begin{algorithmic}[1]
\Procedure{Q-LEARNING}{}
\Ensure {Intialize the $Q$ matrix};
\While {not End Of Learning}
	\State {Visit the state $s$;}
	\State {Select an action $a$ based on an action selection policy;} %\Comment{e.g base on Boltzmann, $\epsilon$-greedy, etc.}
	\State {Carry out the $a$ and observe a reward $r$ at the new state $s'$;}
	\State {$Q[s,a] \gets Q[s,a] + \alpha (r + \lambda \max\limits_{a'}(Q[s',a']) - Q[s,a])$;}
	\State {$s \gets s'$;}
\EndWhile
\State \Return $Q$;
\EndProcedure
\end{algorithmic}
\end{latin}
\end{algorithm}

\fig[1]{rl_schema}{شمایی از فرایند یادگیری تقویتی در تعامل با محیط}

