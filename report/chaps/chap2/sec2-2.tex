\قسمت{اشتراک‌گذاری اطلاعات}
برای اولین بار در\مرجع{tan1993multi} اشتراک‌گذاری داده‌ها در سیستم‌های چند عامله مورد ارزیابی قرار گرفت. هدف این بررسی نمایش اثر اشتراک‌گذاری داده‌ها در مقابل سیستم‌های تک عاملی بود. نتیجه این پژوهش نشان داد که اگر اشتراک‌گذاری به خوبی انجام شود می‌تواند سرعت و کیفیت یادگیری را به‌صورت چشم‌گیری افزایش دهد. در این پژوهش سه نوع اشتراک‌گذاری موردبررسی قرار گرفت در نوع اول که اشتراک‌گذاری ادراک نام گرفت عامل‌ها تنها نتایج مشاهدات خود را به اشتراک می‌گذاشتند، در نوع دوم اشتراک‌گذاری سه‌تایی حالت، عمل، کیفیت اشتراک‌گذاری شده و اشتراک‌گذاری واقعیت نامیده شد و نهایتاً در نوع سوم اشتراک‌گذاری که اشتراک‌گذاری سیاست خوانده می‌شود اطلاعات داخلی عامل‌ها که منبع استخراج سیاست آنهاست به اشتراک گذاشته‌ شده است. اشتراک‌گذاری در این پژوهش با یک میانگین‌گیری ساده بین اطلاعات عامل‌ها انجام می‌شد. در این پژوهش که SA\زیرنویس{\مق{Simple Averaging}} نامیده شد است ثابت‌شده ممکن است اشتراک‌گذاری سربارهایی در ترکیب داده‌ها به سیستم بیفزاید یا در شروع یادگیری از سرعت یادگیری بکاهد اما در طول یادگیری این سربارها جبران شده و اشتراک داده‌ها می‌تواند به‌صورت چشم‌گیری در افزایش سرعت سیستم‌های چند عامله مؤثر باشد.


\قسمت{یادگیری مشترک}
برنجی و همکاران در \سال{1999} روشی تحت عنوان یادگیری مشترک مطرح کردند\مرجع{berenji1999cooperation}. در این روش اشتراک‌گذاری با در نظر گرفتن تنها یک سیاست برای تمام عامل‌ها انجام شد .نتایج این پژوهش نشان می‌دهد که در دسته بزرگی از مسائل روش‌های یادگیری مشترک می‌تواند مفیدتر از روش‌های یادگیری مستقل باشد. فرآیند یادگیری در این روش به این صورت است که عامل ها در محیط اقداماتی انجام می‌دهند و بعد ازدریافت پاداش عمل بروزرسانی را در یک داده مشترک انجام می‌دهند و در انتخاب عمل نیز از همان داده مشترک بهره می‌برند. این به این معنی است که عامل ها دیگر برای خود داده مستقلی ندارند. در این پژوهش حتی یادگیری با منطق فازی ادغام‌شده است و نویسندگان سعی کردند اثر فازی کردن داده‌ها در یادگیری مشارکتی را نمایش دهند.

\قسمت{تقلید}
انسان در طول زندگی برای رسیدن به یادگیری روش‌های متفاوتی دارد. گاهی برای رسیدن به یادگیری باید آزمایش کرد گاهی تحلیل کرد و گاهی تجربه اما یک روش که انسان از آن مخصوصاً در مراحل رشد بسیار بهره می‌برد تقلید است. همین موضوع باعث شده که در یادگیری مشارکتی نیز به تقلید عامل‌ها از هم توجه شود. بر همین اساس نونس و همکاران با ایده برداری از تقلید در انسان پیشنهاد کردند که رابطه عامل‌ها از طریق تقلید از یکدیگر باشد\مرجع{kuniyoshi1994learning}.

موضوع دیگری که در مورد تقلید عامل‌های انسانی باید در نظر گرفته می‌شد این است که عامل‌های انسانی از عامل‌های انسانی تقلید می‌کنند که اطلاعات بیشتری دارند. در پیاده‌سازی انجام‌شده نیز بر همین اساس سه نوع تقلید پیشنهاد می‌شود. تقلید می‌تواند به‌صورت ساده باشد. پیشنهاد داده‌شده است که عامل‌ها همیشه از عامل‌های همسایه (همسایگی در این روش بر اساس همسایگی محلی است چراکه عامل‌هایی که در منطقه یکسانی قرار دارند کمک بیشتری می‌توانند به هم کنند) خود تقلید نمایند. این موضوع یک دور در عامل‌ها ایجاد می‌کند که هر عامل منتظر می‌ماند تا عامل دیگر حرکتی انجام دهد. برای رفع این موضوع نوع دیگری از تقلید به نام تقلید شرطی مطرح می‌شود در تقلید شرطی عامل از کسانی تقلید می‌کند که عملکرد بهتری نسبت به او داشته‌اند در این حالت موضوع دور و انتظار عامل‌ها برطرف شده است. اما درروش سوم که تقلید انطباقی نام دارد عامل همیشه تقلید نکرده و تقلید بر اساس یک احتمال انجام می‌شود.

\قسمت{حافظه جمعی}
گارلند و همکاران در \سال{1996} ایده جدید خود را با عنوان یادگیری حافظه جمعی مطرح کردند\مرجع{garland1996multiagent, garland1995preparation}. در یادگیری حافظه جمعی که برگرفته از شناخت توزیع‌شده در علوم اجتماعی می‌باشد عامل‌ها تجارب خود را در یک حافظه مشترک نگهداری می‌کنند. هر عامل در زمان برخورد با مشکلات می‌تواند با بهره‌گیری از این تجارب راه درست را پیدا کند.
این روش در دو دیدگاه مورد ارزیابی قرارگرفته است. در دیدگاه اول عامل‌ها الگوهای موفق خود در طول یادگیری را در حافظه مشترک نگهداری می‌کنند تا در زمان نیاز تمام عامل‌ها با استفاده از این الگوها بتوانند راه‌حل مشکلات خود را پیدا کنند. در دیدگاه دیگر احتمال موفقیت عامل‌ها نگه‌داری می‌شود که با بهره‌گیری از این داده می‌توان میزان موفقیت عامل‌ها در اعمال مختلف را ارزیابی کرده و در جهت بهبود طراحی سیستم مورد ارزیابی قرارداد. لازم به ذکر است که در این پژوهش‌ها حافظه جمعی را در دو حالت حافظه مرکزی و حافظه توزیع‌شده بین عامل‌ها مورد ارزیابی قرار داده است.

\قسمت{پند}
در \سال{2002} نونس و همکاران باردیگر روشی جدید با عنوان پند دهی مطرح کردند\مرجع{nunes2002learning}. در جوامع انسانی پند دادن بسیار رواج داشته و در زمان مشکلات بسیار کارا می‌باشد. یک عامل انسانی در زمان برخورد با مشکلات از عامل‌هایی که اطلاعات بیشتری دارند پند گرفته و مشکلات خود را حل می‌کند. عاملی انسانی که دارای اطلاعاتی است هم اطلاعات خود را باتجربه کردن و یا گرفتن پند در زمان‌های دیگر به‌دست می‌آورد. مشخصاً یادگیری تقویتی در حالت معمول با تجارب به یادگیری می‌رسد. اگر هر تجربه را بازخوردی از محیط در نظر بگیریم هر پند را نیز می‌توان بازخوردی از عامل‌های دیگر دانست. با این ایده دیگر حتی نیازی نیست که عامل‌ها از روش‌های یکسانی در یادگیری بهره ببرند زیرا پند دادن به عامل‌ها را می‌توان فارغ از روش یادگیری پیاده‌سازی کرد.
ایده پردازان پند در\مرجع{nunes2005advice} کار قبل خود را کامل‌تر کرده و این ایده را به‌صورتی که عامل‌ها در یک محیط به تعامل می‌پرداختند پیاده‌سازی کردند. هر عامل بعد از رسیدن به هر حالت موقعیت خود را به عامل‌های دیگر ارسال می‌نماید. عامل‌هایی که تجربه مشابهی داشته‌اند در پاسخ مقداری را به عنوان میزان ارزش عمل انجام شده برای عامل ارسال می‌کنند و عامل از این مقادیر همانند پاداش دریافتی از محیط بهره می‌برد.

\قسمت{یادگیری مشارکتی بر مبنای خبرگی}
تشریح یادگیری مشارکتی بر مبنای خبرگی را با یک سؤال می‌توان آغاز کرد. آیا عامل‌ها در شناخت محیط از خبرگی یکسانی برخوردار هستند؟ مسلماً چنین نیست، در\مرجع{ahmadabadi2000expertness} ایده یادگیری مشارکتی بر مبنای خبرگی با عنوان WSS\زیرنویس{\مق{Weighted Strategy Sharing}} مطرح می‌شود.
همان‌طور که در تشریح روش SA مطرح شد در این روش با میانگین‌گیری از اطلاعات عامل‌ها ترکیب انجام می‌شود. در این میانگین‌گیری تمام عامل‌ها به یک اندازه سهیم هستند. ایده پردازان WSS با طرح این موضوع که میزان خبرگی عامل‌ها یکسان نیست سعی کردند هر عامل در ترکیب داده‌ها به میزان توانایی و خبرگی خودش مؤثر باشد.

نویسندگان با ارائه معیارهایی میزان خبرگی عامل‌ها را سنجیده و بر همین اساس داده‌ها باهم ترکیب می‌شوند. در WSS روال یادگیری به دو فاز یادگیری مستقل و یادگیری مشارکتی شکسته شده است.
در یادگیری مستقل هر عامل به‌طور مستقل به یادگیری می‌پردازد این یادگیری منجر به کسب اطلاعاتی می‌شود که در فاز یادگیری مشارکتی باهم ترکیب می‌شوند. یادگیر در فاز یادگیری مستقل چندین چرخه یادگیری را تجربه می‌کند. تعداد این چرخه‌ها می‌تواند در بین عامل‌ها یکسان و یا متفاوت باشد. اما باید در انتخاب تعداد چرخه‌های یادگیری هر فاز یادگیری مستقل دقت کرد چراکه اگر این تعداد کم در نظر گرفته شود عامل اطلاعات کافی را جمع آوری نکرده است و اگر زیاد در نظر گرفته شود از تأثیر یادگیری مشارکتی خواهد کاست.

در فاز دوم یادگیری عامل‌ها باید به یادگیری مشارکتی بپردازند. در آغاز این فاز میزان خبرگی عامل‌ها سنجیده می‌شود و پس از ان داده‌ها ترکیب‌شده و جداول $Q$ عامل‌ها بروز رسانی می‌شود. در\مرجع{ahmadabadi2000expertness} روش‌هایی جهت ترکیب دادها ارائه‌ شده است. در یکی از روش‌ها جدول تمام عامل‌ها با بهره‌گیری از میزان خبرگی میانگین‌گیری شده و جدول تولیدشده به تمام عامل‌ها داده شود که در صورت انجام این کار بعد از فاز یادگیری مشارکتی تمام عامل‌ها جدول $Q$ یکسانی خواهند داشت. درروش دیگری پیشنهادشده که هر عامل جدول جدید خود را با ترکیب جدول خود با جدول عامل‌های خبره‌تر از خودش تولید کند. در این ترکیب نیز هر عامل به میزان خبرگی خودش در ترکیب داده‌ها سهم خواهد داشت.

در WSS با در نظر گرفتن خبرگی عامل‌ها تأثیر زیادی در بهبود یادگیری مشارکتی داشته است اما نکته‌ای که در نظر گرفته نشده است اینجاست که میزان خبرگی عامل‌ها در دامنه‌های مختلف بسیار متفاوت بوده و بهتر است که در ترکیب داده‌ها این دامنه‌ها هم در نظر گرفته شود. در \مرجع{eshgh2002extension} با در نظر گرفتن دامنه خبرگی عامل‌ها سعی شده تا نقصان WSS برطرف شود. بعدازآن در\مرجع{ritthipravat2006comparison} سعی شده تا استفاده از جدول $Q$ یک عامل در ترکیب داده‌ها قطعی نباشد. در این راستا در فاز ترکیب برای اطلاعات هر عامل احتمالی در نظر گرفته‌شده است که نشان‌دهنده احتمال حضور اطلاعات آن عامل در ترکیب داده‌ها است. میزان این احتمال نیز بر اساس تفاوت میزان خبرگی عامل‌ها محاسبه‌شده است. در ادامه تعدادی از معیارهای خبرگی معرفی‌شده در\مرجع{ahmadabadi2000expertness} خواهد آمد.
\begin{itemize}
\فقره \textbf{معیار خبرگی معمولی:} در این معیار میزان خبرگی عامل‌ها بر اساس مجموع پاداش‌های دریافتی آنها در نظر گرفته‌شده است. درنتیجه عاملی که میزان پاداش منفی کمتر و میزان پاداش مثبت بیشتری گرفته است را عامل خبره‌تر می‌داند.
\فقره \textbf{معیار خبرگی مثبت:} در این معیار سعی شده با شمارش پاداش‌های مثبت عامل‌ها میزان خبرگی اندازه‌گیری شود. ایده انتخاب این معیار این بوده که عاملی که پاداش مثبت بیشتری گرفته است از خبرگی بالاتری برخوردار است.
\فقره \textbf{معیار خبرگی منفی:} این معیار برعکس معیار خبرگی مثبت با این ایده که عاملی که پاداش منفی بیشتری دارد نقاط بحرانی بیشتری را می‌شناسد عمل شده و تعداد پاداش‌های منفی عامل یادگیری را شمارش می‌نماید.
\فقره \textbf{معیار خبرگی قدر مطلق:} در معیار خبرگی قدر مطلق میزان خبرگی عامل با محاسبه مجموع قدر مطلق پاداش‌ها دریافتی او انجام می‌شود. درنتیجه به پاداش‌های منفی و مثبت ارزش یکسانی داده‌شده است.
\فقره \textbf{معیار خبرگی گرادیان:} در این معیار ماننده معیار اول عمل می‌شود با این تفاوت که میزان افزایش سیگنال دریافتی نسبت به آخرین دوره‌ی یادگیری مشارکتی را معیار خبرگی قرار داده‌ است، هرچقدر این اختلاف بیشتر و مثبت باشد نشان می‌دهد که عامل نسبت به دوره‌ی قبل خبره‌تر شده است.
\فقره \textbf{معیار خبرگی میانگین تعداد قدم‌ها:} این معیار برعکس پنج معیار دیگر به‌جای تأکید بر روی پاداش‌ها میانگین تعداد قدم‌های عامل در چرخه‌های یادگیری را معیار می‌داند. این انتخاب با این ایده انجام‌شده که عامل‌های خبره‌تر با تعداد قدم‌های کمتر چرخه‌های یادگیری را به اتمام می‌رسانند.
\end{itemize}

\قسمت{یادگیری مشارکتی بر مبنای تخته‌سیاه}
در\مرجع{yang2007cooperative} سازوکار تخته‌سیاه مطرح شد. تخته‌سیاه یک حافظه مرکزی است که تمام عامل‌ها به آن دسترسی دارند. در این روش عامل‌ها به‌طور مستقیم باهم ارتباط نداشته و ارتباطات از طریق همین تخته‌سیاه انجام می‌شود. هر عامل می‌تواند بر روی تخته نوشته و یا از آن بخواند.
درروش پیشنهادشده در\مرجع{yang2007cooperative} به این شکل است که عامل بعد از رسیدن به هر موقعیت حالت خود را به تخته‌سیاه اعلام می‌کند و تخته‌سیاه عملی را بر اساس حالت جاری به عامل برمی‌گرداند. عامل بعد از انجام آن عمل و دریافت بازخورد از محیط این بازخورد را به تخته‌سیاه برمی‌گرداند.

تخته‌سیاه دودسته از داده‌ها را نگه‌داری می‌کند. دسته اول داده‌ها همان جدول $Q$ عامل‌ها است و دسته دوم از داده‌ها عمل‌های انجام‌شده توسط هر عامل است. همان‌طور که مشخص است در این روش بروز رسانی جدول $Q$ و انتخاب عمل از عامل به تخته‌سیاه منتقل‌شده و مشخصاً جدول $Q$ باید در تخته‌سیاه پردازش شود. اما دسته دوم اطلاعات صرفاً جهت کمک به انتخاب عمل عامل‌ها انجام می‌شود. به‌عنوان‌مثال اگر عامل در حالتی قرار گیرد و عملی تجربه نشده باشد آن عمل پیشنهاد می‌شود. پس ذخیره‌سازی دسته دوم اطلاعات در جهت مدیریت اکتشاف و بهره‌برداری عامل‌ها از اطلاعات است. در شکل \ref{fig:black_board} مکانیسم تخته‌سیاه نمایش داده‌شده است.

\fig[0.6]{black_board}{شماتیک مکانیزم روش تخته‌سیاه برای یادگیری تقویتی مشارکتی\مرجع{mohammad2015speedup}}

\قسمت{یادگیری تقویتی تعاملی}
در \سال{2006} لیما و همکاران سه الگوریتم به نام‌های \مق{BEST-Q, AVG-Q} و \مق{PSO-Q} را ارائه دادند که این دسته از الگوریتم‌ها مساله‌ی یادگیری مشارکتی تقویتی را به صورت جستجوی‌ جدول Q توسط عامل‌ها تعریف کرده است\مرجع{iima2006reinforcement}. لیما و همکاران ابتدا معیاری ارائه دادند برای سنجش میزان خوب بودن مقادیر جداول Q که در واقع متشکل از مقادیر پاداش‌های دریافتی عامل‌ها می‌باشد، که این معیار شامل جمع تخفیف یافته\زیرنویس{\مق{Discounted Sum}} پاداش‌ها در هر چرخه‌ی یادگیری می‌باشد به‌صورتی که به پاداش‌های نهایی ارزش بیشتری می‌دهد. سپس با استفاده از این معیار به جستجوی بهترین جدول Q از طریق عامل‌ها می‌پردازد؛ که این مساله وجه تمایز کار لیما با روش پیشنهادی در این پژوهش می‌باشد. الگوریتم \کادربی{BEST-Q} در مرحله‌ی به اشتراک‌گذاری دانش، بهترین دانش (با توجه به معیار معرفی شده) را به عنوان دانش جمعی در نظر می‌گیرد. الگوریتم \کادربی{AVG-Q} بهترین دانش را با دانش هر عامل میانگین‌گیری می‌کند و به عنوان دانش همان عامل در نظر می‌گیرد و در الگوریتم \کادربی{PSO-Q} الگوریتم PSO\مرجع{kennedy2011particle} را به عنوان مدل جستجو کننده‌ دانش جمعی در نظر گرفته است.\بند
یکی از معایب این روش‌ها این است که یادگیری تقویتی را به صورت یک مساله‌ی جستجوی مقادیر جداول Q در نظر گرفته است و از آنجایی که ماهیت الگوریتم‌ها به صورت جستجو می‌باشد، اثباتی جهت اینکه این جستجو‌های مقادیر جداول Q به مقادیر بهینه یعنی جدول $Q^*$ همگرا خواهند شد، وجود ندارد. از دیگر معایب این روش‌ها نحوه‌ی محاسبه‌ی میزان بهینگی مقادیر جداول عامل‌ها می‌باشد، بطوری که این روش‌ها به پاداش‌های دریافتی در اواخر چرخه‌ی یادگیری عامل‌ها ارزش بیشتری می‌دهند، این ممکن است در ابتدای یادگیری که عامل‌ها در اوایل چرخه‌ی یادگیری خود حرکت‌های بیهوده‌ی زیادی انجام دهند منطقی به نظر بیاید، ولی بعد از آنکه تعدادی چرخه‌ی یادگیری سپری شد و عامل به دانش نسبی خوبی از محیط خود دست یافت حرکت‌های ابتدایی به اندازه‌ی حرکت‌های نهایی ارزش دارند (و البته شاید هم ارزش بیشتری داشته باشند) زیرا که عامل برای دست یافتن به هدف، نسبت به محیط‌ اطراف اهداف شناخت بهتری دارد (به علت خاصیت شوک یادگیری تقویتی\مرجع{mohammad2015speedup}) که در این شرایط در حالت کلی میزان بهینگی عامل‌ها را بیشتر میزان بهینگی اعمال در اوایل چرخه‌ی یادگیری عامل تعیین می‌کند. معیاری که روش‌های فوق‌الذکر از آن، جهت سنجش خبرگی عامل‌ها در نظر گرفته‌اند این مساله را نادیده می‌گیرد که باعث می‌شود خبرگی عامل‌ها را نتوان به درستی تعیین کرد.

\قسمت{یادگیری مشارکتی بر مبنای پختگی سیاست}
یانگ و همکاران در \سال{2009} روشی با عنوان یادگیری مشارکتی بر مبنای خبرگی چند معیاری ارائه‌ دادند\مرجع{yang2009cooperative}. این روش تا حدودی ترکیب روش تخته‌سیاه با WSS هست. در این روش عامل‌ها حافظه مرکزی خود یا تخته‌سیاه را دارند که وجود تخته‌سیاه عامل‌ها را از شکستن بازه یادگیری به دو فاز بی‌نیاز می‌سازد. در روشی چون WSS یادگیری به دو فاز یادگیری مستقل و یادگیری مشارکتی شکسته می‌شد تا عامل‌ها دادهای خود را به اشتراک بگذارند اما زمانی که عامل‌ها دائما می‌توانند دادهای خود را بر روی تخته‌سیاه نوشته و بخوانند ارتباط از طریق همین تخته‌سیاه انجام خواهد شد.

اما عامل‌ها بر خلاف روش تخته‌سیاه از ارتباط مستقیم هم در تصمیم‌گیری‌ها و انتخاب اعمال بهره می‌برند. عاملی که در وضعیت انتخاب عمل قرار گرفته می‌تواند از عامل‌های دیگر بیاموزد. در این روش جهت شناخت عامل‌هایی که اطلاعات خوبی دارند و می‌توانند آموزگار باشند از معیارهای خبرگی ارائه‌شده در WSS استفاده‌شده است. با این کار عامل از عامل‌هایی می‌آموزد که واقعاً از خبرگی بالاتری برخوردار هستند. این کار باعث می‌شود که در شروع یادگیری که عامل‌ها داده کمی دارند و نیز عامل آموزگاری نداشته با کمک اطلاعات و دستورات تخته‌سیاه عمل کند و بعد طی مراحلی از یادگیری که عامل‌ها داده‌های زیادی کسب کردند با بهره بردن از نظرات دیگر عامل‌ها انتخاب‌های بهتری داشته باشند.

\قسمت{یادگیری مشارکتی بر مبنای خبرگی چند معیاری}
پاکیزه و همکاران در \سال{2013} با نقد روش WSS روشی جدید ارائه کردند\مرجع{pakizeh2013multi}. ایشان با اشاره به این موضوع که خبرگی در یک‌رشته نبوده در کار خود از ترکیب 6 معیار خبرگی WSS در کنار هم بهره برده‌اند. ایشان تأکیددارند که عامل‌های انسانی درزمینه‌های مختلف خبرگی‌های متفاوتی دارند و این موضوع در عامل‌های هوشمند نیز وجود دارد. این پژوهش هر یک از معیارهای ارائه‌شده در WSS را مانند یک زمینه در عامل انسانی دانسته و درروش خود از تمام این معیارها در کنار هم بهره برده‌اند.

پاکیزه و همکاران مانند WSS یادگیری را در دو فاز یادگیری مستقل و یادگیری مشارکتی تقسیم می‌نمایند عامل‌ها در فاز یادگیری مشترک از هر معیار برای ترکیب داده‌های جدول Q بهره می‌برند و بعد از ترکیب جدول به‌وسیله هر معیار 6 جدول مشارکتی تولید می‌شود که هر یک بر اساس یک معیار خبرگی است. آن‌ها برای ترکیب این جداول آنها را باهم جمع می‌کنند. اما موضوعی که وجود دارد این است که جدول تولیدشده به‌وسیله جمع چندین جدول دیگر خواص جدول $Q$ را ندارد. برای رفع این مشکل این جدول را نه در جایگزینی با جدول $Q$ عامل‌ها بلکه در کنار جدول $Q$ عامل نگه‌داری می‌نمایند. به عبارت دیگر هر عامل دو جدول دارد یک جدول $Q$ که بر اساس یادگیری تقویتی است و جدول دیگر که جدول مشارکتی عامل‌ها است. پاکیزه و همکاران پیشنهاد کردند که از جدول مشارکتی که خواص جدول $Q$ عامل‌ها را ندارد صرفاً برای انتخاب عمل استفاده شود و عامل بر اساس این جدول عمل را انتخاب کرده انجام دهد سپس جدول $Q$ خود را بروز رسانی نماید. جهت درک بهتر این روش شمای کلی آن در شکل \ref{fig:coop_learning} آورده شده است.

\fig[.8]{coop_learning}{شمایی از یادگیری مشارکتی برمبنای خبرگی عامل‌ها\مرجع{thesis:pakizeh2013multi}}

\قسمت{تسریع یادگیری مشارکتی با بهره‌گیری از کوتاه‌ترین فاصله تجربه‌شده}
میرزایی در \سال{2016} جهت تسریع در یادگیری مشارکتی دو معیار جدید را ارائه کرد\مرجع{mohammad2015speedup}. معیار اول یک معیار مکاشفه است که کوتاه‌ترین فاصله تجربه‌شده توسط عامل از هر حالت و عمل را شمارش می‌کند. ایشان نام این معیار را SEP\زیرنویس{\مق{Shortest Experienced Path}} گذاشته است. معیار دیگر که شوک نام‌گذاری شده است میزان شناخت عامل از هر حالت و عمل را محاسبه می‌نماید.

میرزایی برخلاف دیگران فقط در فاز ترکیب داده‌های یادگیری مشارکتی تغییر ایجاد نکرده است. وی در فاز انتخاب عمل توسط عامل‌های مشارکتی نیز از جدول SEP در کنار جدول Q استفاده کرده است. استدلال ایشان در انجام این کار چنین بوده که عامل‌های یادگیری تقویتی در فازهای اول یادگیری داده زیادی ندارند و ازآنجایی‌که جدول SEP با سرعت بیشتری به‌روزرسانی می‌شود بهتر است انتخاب اعمال در فازهای اولیه یادگیری بیشتر بر اساس SEP انجام شود. ایشان با استفاده از شوک که نمایشی از میزان شناخت عامل از هر حالت و عمل است تعادلی بین بهره‌برداری از جدول SEP و جدول $Q$ برقرار کرده است. در شروع یادگیری که شناخت عامل کمتر است بیشتر انتخاب بر اساس SEP انجام می‌شود و در طول یادگیری با افزایش میزان شناخت عامل از محیط انتخاب عمل بر اساس جدول $Q$ افزایش می‌یابد.

 همچنین میرزایی در پژوهش خود در فاز ترکیب داده‌ها نیز روش جدیدی ارائه داد. ازآنجایی‌که وی یک جدول جدید به سیستم افزوده است در فاز ترکیب داده‌ها جدول SEP عامل‌ها را نیز ترکیب می‌نماید. همچنین جداول SEP عامل‌ها را تنها با یک حداقل‌گیری باهم ترکیب کرده و به عامل‌ها برمی‌گرداند. سپس ترکیب جداول $Q$ عامل‌ها به‌صورت محلی انجام می‌گیرد به این صورت که هر سطر از جدول که نمایش یک حالت از محیط است به‌صورت جداگانه بروز رسانی می‌شود. ایشان در ترکیب داده‌های هر سطر عامل‌ها را به دو گروه تقسیم نموده و داده‌های هر گروه را جداگانه ترکیب می‌نماید. این تقسیم‌بندی بر اساس رابطه بین سیاست‌های استخراج‌شده از جدول $Q$ و SEP عامل در یک حالت هست. وی عامل‌هایی که سیاست استخراج‌شده از جدول $Q$ و SEP در آنها همخوانی داشته باشد در یک گروه و عامل‌هایی که سیاست استخراج‌شده آنها عمل‌های متفاوتی را پیشنهاد می‌کنند را در گروه دیگر قرار داده است. ترکیب داده‌های هر گروه با استفاده از میزان شناخت عامل از آن حالت (شوک) انجام می‌شود به این صورت که داده‌های عملی که شناخت بیشتری دارند بیشتر مورداستفاده قرار می‌گیرند. در فصل بعد روش محاسبه ارائه شده توسط میرزایی تشریح شده است.