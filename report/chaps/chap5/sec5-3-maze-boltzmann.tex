\زیرزیرقسمت{سیاست انتخاب عمل «بولتزمن»}
\پاراگراف{مقایسه در سرعت و کیفیت یادگیری:} نتایج حاصل از اجرای الگوریتم‌ها در محیط پلکان مارپیچ در شکل
\ref{fig:boltzmann/maze.pref.compare.png}
آمده است. در این شکل محور افقی تعداد تلاش‌های یادگیری عامل را نشان می‌دهد که در تلاش اول عامل بدون دانش اولیه شروع به تعامل با محیط می‌کند و در تلاش ۱۰۰۰ام عامل به اجرای خود پایان می‌دهد. محور عمودی نموار میانگین تجمعی تعداد قدم‌های عامل را نشان می‌دهد. اعداد کناری برچسب‌ها (گوشه بالا سمت راست) متوسط تعداد قدم‌ در آخرین تلاش عامل می‌باشد که انتظار می‌رود عامل آگاهی نسبی کاملی از محیط دارد را نشان می‌دهد که این عدد هرچقدر کمتر باشد نشان می‌دهد که عامل در طی رسیدن به هدف تعداد گام کمتری برداشته است و در نتیجه دانش و شناخت بهتری از محیط دارد.

\fig[.8]{boltzmann/maze.pref.compare.png}{مقایسه در سرعت و کیفیت یادگیری در محیط پلکان مارپیچ با تابع بولتزمن}

همانطور که مشاهده می‌شود روش SEP دارای ۲٪ بهبود نسبت به IL می‌باشد در حالی که روش پشنهادی در زمانی که از انتگرال فازی استفاده می‌کند در بدترین حالت دارای ۱۸٪ بهبود و در بهترین حالات دارای ۳۳٪ بهبود می‌باشد که نسبت به روش SEP تقریبا ۹ الی ۱۶ برابر نتیجه را بهبود داده است. در صورتی که از میانگین وزنی بجای انتگرال فازی استفاده شود نتایج با اختلاف اندکی (کمتر از ۱-٪) \زیرخط{بدتر} از یادگیری IL بوده است که نشان ‌می‌دهد که استفاده از انتگرال فازی چقدر می‌تواند نسبت به روش‌های سنتی و معمولی چون میانگین وزنی موثر واقع شود. نتایج این قسمت را می‌توان در جدول
\ref{tab:maze_pref_compare}
خلاصه کرد.

\begin{table}
\centering
\caption{مقایسه در میزان بهبود کیفیت یادگیری در محیط پلکان مارپیچ با تابع بولتزمن}\label{tab:maze_pref_compare}
\begin{latin}
\begin{tabular}{|*8{l|}}
\hline
\multicolumn{3}{|c|}{}& \multicolumn{5}{c|}{REFMAT}
\\\hline
& IL & SEP & wsum & fci-mean & fci-max & fci-k-mean & fci-const-one  
\\\hline
IL & \%0.0 & & & & & &  
\\\hline
SEP & \%2.2 & \%0.0 & & & & &  
\\\hline
wsum & \%-0.2 & \%-2.3 & \%0.0 & & & &  
\\\hline
fci-mean & \%14.9 & \%12.5 & \%15.1 & \%0.0 & & &  
\\\hline
fci-max & \%18.0 & \%15.5 & \%18.2 & \%2.7 & \%0.0 & &  
\\\hline
fci-k-mean & \%24.0 & \%21.4 & \%24.2 & \%7.9 & \%5.1 & \%0.0 &  
\\\hline
fci-const-one & \%33.6 & \%30.7 & \%33.8 & \%16.2 & \%13.2 & \%7.7 & \%0.0  
\\\hline
\end{tabular}
\end{latin}
\end{table}

\پاراگراف{مقایسه در پیچیدگی زمانی:} در این قسمت به مقایسه‌‌ی پیچیدگی زمانی روش پیشنهادی با روش SEP مورد بررسی قرار می‌گیرد، برای محاسبه‌ی پیچیدگی زمانی به روش ریاضی کار بسیار دشوار و پرخطایی می‌باشد؛ در اینجا ما بجای محاسبه‌ی پیچیدگی زمانی ریاضی دو الگوریتم از مدت زمانی که طول می‌کشد برنامه در سیستم اجرا و خاتمه یابد استفاده می‌کنیم. در شکل
\ref{fig:boltzmann/maze.time.compare.png}
میانگین زمانی ۲۰ اجرای مستقل برحسب میلی‌ثانبه به ازای هریک از تعداد تلاش‌ها آورده شده است. همان‌طور که در این شکل مشاهده‌ی می‌شود الگوریتم IL دارای حداکثر سرعت اجرا می‌باشد زیرا که هیچ سربار محاسباتی یادگیری مشترک را ندارد؛ هدف یادگیری اشتراکی این است که می‌خواهد در ازای یک سری سربار محاسباتی کیفیت و سرعت «یادگیری» عامل‌ها را افزایش دهد. با در نظر داشتن این موضوع همانطور که قبلا دیدیم روش پیشنهادی سرعت و کیفیت یادگیری را بیشتر از روش SEP افزایش می‌دهد و در اینجا نیز می‌بینیم که دارای پیچیدگی زمانی کمتری نسبت به روش SEP می‌باشد که نشان از بهینه‌گی روش پیشنهادی نسبت به روش SEP می‌دهد.

\fig[0.6]{boltzmann/maze.time.compare.png}{مقایسه در پیچیدگی زمانی روش‌ها به ازای تعداد تلاش‌های متفاوت برحسب میلی‌ثانیه}

\پاراگراف{مقایسه در میزان باروری:}
\begin{definition}[سرعت باروری]\setstretch{\thebaselinestretch}\label{def:fertility_speed}
اگر فرض کنیم الگوریتم یادگیری تقویتی $\psi_Q(\mathcal{E})$ وجود دارد که در محیط $\mathcal{E}$ فعالیت می کند و دانش خود را در جدولی مانند $Q$ ذخیره می‌کند، سرعت باروری الگوریتم $\psi_Q(\mathcal{E})$ را سرعت همگرایی حداکثر مقدار جدول $Q$ به سمت حداکثر پاداش محیط قابل دریافت تعریف می‌کنیم.
\end{definition}
\begin{definition}[میزان باروری]\setstretch{\thebaselinestretch}\label{def:fertility_rate}
انتگرال سرعت باوری را میزان باروری الگوریتم $\psi_Q(\mathcal{E})$ که در محیط $\mathcal{E}$ فعالیت می کند و دانش خود را در جدولی مانند $Q$ ذخیره می‌کند، تعریف می‌کنیم.
\end{definition}

\begin{theorem}[معیاری جدید برای سرعت یادگیری]\setstretch{\thebaselinestretch}\label{theorem:learning_speed}
طبق تعاریف \ref{def:fertility_speed} و \ref{def:fertility_rate} الگوریتمی میزان باروری بیشتری دارد که سریع‌تر مقادیر جدول $Q$ خود را به سمت بیشنه مقداری که می‌توانند داشته باشد(یعنی بیشنه پاداشی که از محیط می‌توانند کسب کند) سوق دهد. معمولا این در الگوریتم‌های یادگیری تقویتی $Q$ این کار با تنظیم مقدار سرعت یادگیری $\alpha$ صورت می‌گیرد که باعث می‌شود الگوریتم‌ها با سرعت بیشتری به یادگیری نحوه‌ی تعامل با محیط بپردازند. لذا در شرایط یکسان می‌توان گفت الگوریتمی بهتر عمل می‌کند که نحوه‌ی تعامل با محیط را سریع‌تر نسبت به دیگر الگوریتم‌ها یاد می‌گیرید و میزان باروری بیشتری داشته باشد.
\end{theorem}

در شکل
\ref{fig:boltzmann/maze.qtable.max.compare.png}
آورده شده است حداکثر میزان جدول $Q$ روش‌ها در هر تلاش آورده شده است. همانطور که قبلا در تعریف محیط پلکان مارپیچ آورده شده است حداکثر مقدار پاداش این محیط مقدار ۱۰ می‌باشد لذا همان‌طور که مشاهده می‌شود الگوریتم‌ها با شیب‌های متفاتی حداکثر مقدار جداول خود را به سمت حداکثر مقدار پاداش قابل دریافت از محیط سوق می‌دهند. در این شکل سرعت باروری شیب نمودار در هر تلاش می‌باشد و میزان باروری مساحت زیر نمودار می‌باشد.

\fig[.8]{boltzmann/maze.qtable.max.compare.png}{نمودار باروری الگوریتم‌ها مختلف}

در شکل
\ref{fig:boltzmann/maze.qtable.max.compare.png}
منظور از \مق{RAND-WALK} حرکت کاملا تصادفی می‌باشد، به این صورت که عامل بعد از هر حرکت جدول $Q$ خود را بروز رسانی می‌کند ولی هنگام انتخاب عمل در تابع بولتزمن مقدار $\tau \rightarrow +\infty$ در نظر گرفته می‌شود تا میزان احتمال تمامی حرکت‌ها یکسان شود و در نتیجه حرکتی به صورت تصادفی انتخاب شود. همانطور که در قسمت‌های قبل دیدیم روش پیشنهادی هم در کیفیت و هم در سرعت یادگیری بهبود چشم‌گیری دارد و از طرفی هم در نمودار
\ref{fig:boltzmann/maze.qtable.max.compare.png}
دارای بیشترین میزان باروری(مساحت زیرنمودار) حداکثر مقدار جدول $Q$ می‌باشد که این مساله تایید کننده‌ی تئوری
\ref{theorem:learning_speed}
می‌باشد.

دلیل وجود نتایج آزمایش‌ اجرای \مق{RAND-WALK} در این قسمت این است که بررسی کنیم در صورتی که اگر عامل بصورت کورکورانه حرکت کند روش‌ معرفی شده و SEP چقدر در میزان بارور شدن جدول $Q$ عامل‌ها موثرند؟ به عبارت دیگر، در صورتی که استراتژی خاصی جهت انتخاب عمل وجود نداشته باشد، روش‌ها چقدر قدرت باروری دارند؟ همانطور که در شکل
\ref{fig:boltzmann/maze.qtable.max.compare.png}
مشاهده می‌کنیم روش معرفی شده در زمانی که به صورت تصادفی اقدام به انتخاب عمل می‌کند بیشتر از زمانی که IL با استفاده از تابع بولتزمن اقدام به انتخاب عمل می‌کند جدول $Q$ را بارور می‌کند که از قدرت روش ارائه شده خبر می‌دهد. همچنین در مورد روش SEP می‌بینیم که در زمانی که بصورت تصادفی اقدام به عمل می‌کند باروری کمتری نسبت به روش پیشنهادی و IL دارد؛ یعنی میزان باروری روش SEP وابستگی زیادی به سیاست انتخاب عمل دارد و در صورت نداشتن سیاست انتخاب عمل خاصی بشدت عملکردش کاسته می‌شود ولی در روش پیشنهادی میزان این وابستگی از شدت کمتری برخوردار است که از دیگر امتیازات مثبت روش پیشنهادی می‌باشد.

\پاراگراف{مقایسه تاثیر تعداد عامل‌ها میزان کیفیت و سرعت یادگیری:} در این مقایسه سعی شده است که تاثیر یک فاکتور بنیادی سیستم‌های چندعامله مشارکتی را مورد بررسی قرار دهیم، و آن میزان تاثیر پذیری روش‌های مورد مقایسه با افزایش تعداد عامل‌ها می‌باشد. در تئوری سیستم‌های چندعامله مشارکتی دیدگاه معقول براین است که اثر تعداد عامل‌ها در کیفیت و سرعت یادگیری مشارکتی باید مثبت باشد. درغیر این صورت سیستم‌های چندعامله‌ای که تعداد عامل‌ها تاثیری در خروجی سیستم نداشته باشد، دیگر ماهیت سیستم‌های چند عامله را ندارد.

همان‌طور که در شکل
\ref{fig:boltzmann/maze.multi-agent.pref.compare.png}
آمده است، روش پیشنهادی و روش SEP به ازای تعداد عامل‌های ۲، ۳، ۵، ۱۰ و ۲۰ عدد به تعداد ۲۰ بار اجرا درآمده و میانگین اجراها به نمودار کشیده شده است. همانطور که می‌بینیم روش SEP در زمانی ۲۰ عامل در حال یادگیری و اشتراک گذاری دانش‌های خود هستند نسبت به زمانی که فقط ۲ عامل در حال تعامل مشارکتی با محیط هستند فقط ۲٪ در خروجی الگوریتم تاثیر مثبت داشته است. این در حالی است که در همین شرایط میزان بهبود نتیجه‌ی روش پیشنهادی ۵۶٪ می‌باشد. که نشان می‌دهد روش SEP نسبت به افزایش تعداد عامل‌ها رفتاری تقریبا خنثی از خود نشان می‌دهد درحالی که روش پیشنهادی در ازای افزایش تعداد عامل‌ها به دلیل اینکه دانش جمعی نیز افزایش می‌یابد کیفیت خروجی آن نیز بهتر می‌شود.

\fig[.5]{boltzmann/maze.multi-agent.pref.compare.png}{مقایسه تاثیر تعداد عامل‌ها میزان کیفیت و سرعت یادگیری}

\پاراگراف{نتیجه‌گیری:} نتیجه‌ای که از مقایسه‌ی روش پیشنهادی در هر چهار مقایسه‌ی بالا می‌توان گرفت این است که روش پیشنهادی بهبود چشم‌گیری به روش SEP در محیط پلکان مارپیچ و سیاست انتخاب عمل بولتزمن داده است.