\قسمت{یادگیری مشارکتی $Q$ با استفاده از ماتریس ارجاع و انتگرال فازی}
آنچه که تا به اکنون در مورد روش پیشنهادی این پژوهش آورده شده، معرفی یک معیار خبرگی که در برعکس بسیاری از معیارهای خبرگی که تا به کنون معرفی شده است\مرجع{ahmadabadi2000expertness, pakizeh2013multi, mohammad2015speedup} در تمامی موقعیت‌های دنیای واقعی به وفور مشاهده می‌شود و آن ارائه این فرضیه است عامل خبره‌تر برای رسیدن به یک مجموعه از اهداف تلاش نسبی کمتری نسبت به دیگر عامل‌ها با خبرگی کمتر در شرایط یکسان می‌کند. حال که معیاری برای میزان خبرگی عامل‌ها در اختیار داریم چالش بعدی برای بهبود کیفیت و سرعت یادگیری مشارکتی ارائه‌ی روشی برای ترکیب دانش‌های عامل‌ها از محیط (جداول $Q$ آن‌ها) با استفاده از معیار ارائه شده می‌باشد. روش ترکیب باید بگونه‌ای باشد که کیفیت و سرعت یادگیری مشارکتی عامل‌ها را در طی زمان نسبت زمانی که عامل‌ها بدون مشارکت یاد می‌گیرند بهتر کند. همچنین کیفیت و سرعت یادگیری همبستگی مستقیمی داشته باشند با تعداد عامل‌هایی که درحال اشتراک گذاری هستند؛ به عبارت دیگر در صورت افزایش تعداد عامل‌هایی که دانش‌های خود را به اشتراک می‌گذارند مدل ترکیب کننده‌ی دانش‌های آن‌ عامل‌ها باید بتواند دانش‌ بهتری تولید کند که نهایتا منجر به بهتر شدن کیفیت و سرعت کلی یادگیری عامل‌ها شود.\بند
در این پژوهش ما انتگرال فازی را به عنوان مدل ترکیب کننده‌ی دانش‌های عامل‌ها پیشنهاد می‌دهیم. دلیل انتخاب این مدل ویژگی‌های منحصر به فردی است که این مدل کننده در اختیار دارد که مدل را کاملا مناسب برای ترکیب دانش‌ عامل‌ها می‌کند؛ که در بخش‌های آتی فصل این ویژگی‌ها و دلایل مناسب بودن آن‌ها برای ترکیب دانش عامل‌ها آورده شده است. لازم به یاد‌آوری است که همانطور که در قسمت
\ref{SEC:FI_PREVIEW}
این پایان‌نامه آورده شده است ما از به دلایل فنی از انتگرال فازی چوکت استفاده می‌کنیم که در بخش‌ های بعدی این دلایل نیز بطور مفصل شرح داده می‌شود.

\زیرقسمت{الگوریتم پیشنهادی}
در این قسمت به معرفی الگوریتم پیشنهادی می‌پردازیم. آنچه که در الگوریتم
\ref{alg:proposed}
آمده است به دو قسمت تشکیل شده است، یک قسمت که مربوط یادگیری مستقل (خطوط
\ref{alg:proposed:il_start} تا \ref{alg:proposed:il_end})
و قسمت دیگری مربوط به یادگیری مشارکتی (خطوط
\ref{alg:proposed:co_start} تا \ref{alg:proposed:co_end})
می‌باشد. ورودی الگوریتم تعداد عامل‌ها می‌باشد و در ابتدا ماتریس‌های $Q$ و \رفمت\ و \رسمت\ مقداردهی می‌شود. سپس تا زمانی که یادگیری پایان نیافته است ابتدا عامل‌ها در قسمت یادگیری مستقل به صورت جدا گانه در محیط فعالیت می‌کنند که رویه‌های آورده شده در خطوط
\ref{alg:proposed:q_start} تا \ref{alg:proposed:q_end}
همان الگوریتم یادگیری $Q$ متعارف می‌باشد\مرجع{sutton1998reinforcement}. در قسمت یادگیری مستقل تنها خط
\ref{alg:proposed:update_refmat}
می‌باشد که در روش پیشنهادی به شبه‌کد اضافه شده است و این تنها یک وظیفه‌ی بسیار ساده را انجام می‌دهد و آن شمارش میزان حضور عامل در هر کدام از نواحی از پیش تعیین شده است؛ $\phi(\cdot)$ یک تابع نگاشت از یک موقعیت به یک ناحیه از محیط می‌باشد.\بند

\begin{algorithm}[t]\setstretch{1.2}
\caption{\rl{الگوریتم پیشنهادی یادگیری مشارکتی برمبنای ماتریس \رفمت\ و انتگرال فازی}}\label{alg:proposed}
\begin{latin}
\begin{algorithmic}[1]
\Procedure{REFMAT-COOPERATIVE-LEARNING}{$m$}
\Require $m > 1$ \Comment {The number of agents.}
\Ensure {Intialize the $Q$ matrix};
\Ensure {Intialize the $\text{RCMAT} \gets 0$};
\Ensure {Intialize the $\text{REFMAT} \gets 0$};
\While {not End Of Learning}
	\If {In individual learning mode} %\Comment {Individual learing mode for each agent \textbf{independently}.}
		\State {Visit the state $s$;} \label{alg:proposed:q_start}
		\label{alg:proposed:il_start}
		\State {Select an action $a$ based on an action selection policy;} %\Comment{e.g base on Boltzmann, $\epsilon$-greedy, etc.}
		\State {Carry out the $a$ and observe a reward $r$ at the new state $s'$;}
		\State {$Q[s,a] \gets Q[s,a] + \alpha (r + \lambda \max\limits_{a'}(Q[s',a']) - Q[s,a])$;}
		\State {$s \gets s'$;} \label{alg:proposed:q_end}
		\State {Increment $\text{REFMAT}(\phi(s))$ by one;} \label{alg:proposed:update_refmat}
		\label{alg:proposed:il_end}
	\ElsIf {In cooperative learning mode}
		\State $\vec{K} \gets \{\}$; \label{alg:proposed:co_start}
		\State $\vec{R} \gets \{\}$;
		\For {each agent $i \gets 1, m$}
			\State $\text{REFMAT}_i$, $\text{RCMAT}_i$ $\gets$ \Call{Conditional\_Swap}{$\text{REFMAT}_i$, $\text{RCMAT}_i$};
			\State $\vec{K}.add(Q_i)$;
			\State $\vec{R}.add(\text{REFMAT}_i)$;
		\EndFor
		\State $Q \gets$ \Call{FCI\_Combiner}{$\vec{K}$, $\vec{R}$}; \label{alg:proposed:FCI_Combiner}
		\label{alg:proposed:QCO_replacement}
		\State {$\text{REFMAT} \gets 0$};
		\label{alg:proposed:co_end}
	\EndIf
\EndWhile
\EndProcedure
\end{algorithmic}
\end{latin}
\end{algorithm}

بعد از طی یادگیری مستقل عامل‌ها به قسمت اشتراک گذاری دانش‌های خود(جداول $Q$) می‌رسند (خطوط
\ref{alg:proposed:co_start} تا \ref{alg:proposed:co_end}).
در قسمت یادگیری مشترک ابتدا طبق آنچه که در در قسمت آورده شده است جداول \رفمت\ و \رسمت\ به صورت مشترک بروزرسانی می‌شود و سپس جداول $Q$ و \رفمت\ تمامی عامل‌ها به مدل ترکیب کننده فازی معرفی شده در این پژوهش فرستاده می‌شود و مدل ترکیب کننده فازی وظیفه‌ی استخراج یک دانش جدید با در نظر گرفتن ورودی‌های آن برای جایگزینی دانش قابلی عامل‌ها را دارد.

الگوریتم تابع \مق{Conditional\_Swap($\cdot$)} بسیار ساده می‌باشد و مقادیر غیر صفر ماتریس ارجاع را در ماتریس خاطره کپی می‌کند و مقادیر صفر ماتریس ارجاع را از ماتریس خاطره جایگزین می‌کند. این تابع در الگوریتم
\ref{alg:swap_func}
آمده است.

\begin{algorithm}[t]\setstretch{1.2}
\caption{تابع \مق{Conditional\_Swap} معرفی شده در الگوریتم \ref{alg:proposed}}\label{alg:swap_func}
\begin{latin}
\begin{algorithmic}[1]
\Procedure{Conditional\_Swap}{REFMAT, RCMAT}
\Require \Call{size}{REFMAT} = \Call{size}{RCMAT}
	\For {each element $r$ in REFMAT and its corresponding element $c$ in RCMAT}
		\If {$r = 0$}
			\State Update $r = c$;
		\Else
			\State Update $c = r$;
		\EndIf
	\EndFor
	\State \Return REFMAT, RCMAT
\EndProcedure
\end{algorithmic}
\end{latin}
\end{algorithm}

در این پژوهش در دوقسمت نوآوری صورت گرفته است، قسمت اول ارائه‌ی معیاری جدید برای سنجش معیار خبرگی که طبق تعریف
\ref{experties_definition}
این معیار در خط
\ref{alg:proposed:update_refmat}
الگوریتم
\ref{alg:proposed}
پیاده‌سازی شده است؛ نوآوری دوم نحوه‌ی ترکیب اطلاعات دانش عامل‌ها با استفاده از انتگرال فازی که در خط
\ref{alg:proposed:FCI_Combiner}
الگوریتم
\ref{alg:proposed}
و شرح جزییات پیاده‌سازی آن در الگوریتم
\ref{alg:fci_combinator}
آمده است.

\begin{algorithm}[t]\setstretch{1.2}
\caption{تابع \مق{FCI\_Combiner} معرفی شده در الگوریتم \ref{alg:proposed}}\label{alg:fci_combinator}
\begin{latin}
\begin{algorithmic}[1]
\Procedure{FCI\_Combiner}{$\vec{K}$, $\vec{R}$}
\Require \Call{length}{$\vec{K}$} = \Call{length}{$\vec{R}$} = $m$
\Ensure {Initialize $\text{CoQ}_{\text{FCI}}$}
	\For {each state $s$} \label{alg:fci_combinator:line:foreach:state}
		\State $\vec{f} \gets \{\}$; \Comment {Contains the normalized valued of REFMATs' value for state $s$ for all agents}
		\For {each $\text{REFMAT}_i$ in $\vec{R}$}\label{alg:fci_combinator:line:refmat:foreach}
			\State $\vec{f}.add(\text{REFMAT}_i(\phi(s)))$;\label{alg:fci_combinator:line:refmat:foreach:loop}
		\EndFor
		\State $\vec{A} \gets 1 - $ \Call{normalize}{$\vec{f}$}; \label{alg:fci_combinator:line:normalize}
		\For {each possible action $a$ in state $s$} \label{alg:fci_combinator:line:fci:foreach:state_action}
			\State $\vec{x} \gets \{\}$; \Comment {Contains the $Q$ values of action $a$ in state $s$ for all agents}
			\For {each $Q_i$ in $\vec{K}$} \label{alg:fci_combinator:line:fci:foreach:state_action:x}
				\State $\vec{x}.add(Q_i[s, a])$;\label{alg:fci_combinator:line:fci:foreach:state_action:x:loop}
			\EndFor
			\State $\text{CoQ}_{\text{FCI}}[s, a] \gets \sum_{i = 1}^{m} \left( f(x_{\pi_{(i)}}) - f(x_{\pi_{(i-1)}}) \right) \cdot g(\vec{A}_i)$ \Comment{The Choquet Integral} \label{alg:fci_combinator:line:fci}
		\EndFor
	\EndFor
\State \Return $\text{CoQ}_{\text{FCI}}$;
\EndProcedure
\end{algorithmic}
\end{latin}
\end{algorithm}

ورودی‌های الگوریتم
\ref{alg:fci_combinator}
به ترتیب مجموعه‌ای از جداول $Q$ و ماتریس‌های ارجاع(\رفمت) تمامی عامل‌ها می‌باشد بطوری که در ازای هر جدول $Q$ یک ماتریس \رفمت\ متناظر وجود دارد. خروجی این الگوریتم یک جدول $Q$ می‌باشد که از ترکیب جداول $Q$ ورودی با درنظر گرفتن میزان خبرگی هرکدام از عامل‌ها که توسط ماتریس‌های \رفمت\ آن‌ها تعیین می‌شود. الگوریتم
\ref{alg:fci_combinator}
به ازای کلیه‌ی موقعیت‌ها($s$ها در خط \ref{alg:fci_combinator:line:foreach:state}) ابتدا مقادیر \رفمت\ کلیه‌ی عامل‌ها در ناحیه‌ای که آن موقعیت در آن واقع است (که توسط تابع نگاشت $\phi(\cdot)$ بدست می‌آید) را استخراج می‌کند و در برداری بنام $\vec{f}$\زیرنویس{Factors} ذخیره می‌کند (خطوط \ref{alg:fci_combinator:line:refmat:foreach} و \ref{alg:fci_combinator:line:refmat:foreach:loop}) که در واقع میزان ارجاعات هرکدام از عامل‌ها در ناحیه‌ی $\phi(s)$ می‌باشد. بردار $\vec{f}$ معیاری برای سنجش میزان خبرگی کلی عامل‌ها در موقعیت $s$ است، طبق آنچه که در تعریف
\ref{experties_definition}
آمده است در هر ناحیه‌ عاملی خبره‌تر است که مقدار \رفمت\ مربوط به آن ناحیه از دیگر عامل‌ها کمتر باشد. در نتیجه در خط \ref{alg:fci_combinator:line:normalize} بعد از عادی‌سازی\زیرنویس{Normalize} مقادیر \رفمت\ عامل‌ها در ناحیه‌ی $\phi(s)$ یک مکمل‌ گیری صورت می‌گیرد تا عاملی که مقدار \رفمت\ کمتری دارد دارای بیشترین مقدار بعد از عادی‌سازی شود. در خط \ref{alg:fci_combinator:line:fci:foreach:state_action} به ازای کلیه‌ی عمل‌های ممکن در موقعیت $s$ ابتدا مقادیر $Q$ تک‌تک عامل‌ها را در موقعیت $s$ و عمل $a$ در خطوط \ref{alg:fci_combinator:line:fci:foreach:state_action:x} و \ref{alg:fci_combinator:line:fci:foreach:state_action:x:loop} در بردار $\vec{x}$ ذخیره می‌کنیم و در نهایت در خط \ref{alg:fci_combinator:line:fci} با استفاده از انتگرال فازی چوکت معرفی شده در \ref{eq:choquet_integral} مقدار $Q$ مشارکتی حاصل از میزان خبرگی بردار $\vec{A}$ و مقادیر $Q$‌های تک‌تک عامل‌ها در بردار $\vec{x}$ در موقعیت $s$ و عمل $a$ بدست محاسبه می‌شود.

\زیرقسمت{تعیین توابع $f(\cdot)$ و $g(\cdot)$ در انتگرال فازی چوکت}
بطور خلاصه در الگوریتم \ref{alg:fci_combinator} دو بخش عمده دارد بخش اول مربوط استخراج میزان خبرگی عامل‌ها بگونه‌ای که عاملی که خبره‌تر از دارای مقدار خبرگی بیشتری باشد که این بخش در خطوط \ref{alg:fci_combinator:line:refmat:foreach} تا \ref{alg:fci_combinator:line:normalize} صورت می‌گیرد؛ بخش دیگر محاسبه‌ی مقادیر $Q$ مشارکتی کلیه‌ی عمل‌های ممکن در یک موقیت با درنظر گرفتن میزان خبرگی عامل‌ها و مقادیر $Q$ آن‌ها با استفاده از انتگرال فازی چوکت که در خطوط \ref{alg:fci_combinator:line:fci:foreach:state_action} تا \ref{alg:fci_combinator:line:fci} صورت می‌پذیرد.

آنچه که در خط \ref{alg:fci_combinator:line:fci} الگوریتم \ref{alg:fci_combinator} مورد توجه واقع شود این است که توابع $f(\cdot)$ و $g(\cdot)$ چگونه تعریف باید تعریف شوند؟ برای تعیین تابع $f(\cdot)$ منطقی که در این پژوهش استفاده کردیم بدین صورت است که از آنجایی که خروجی تابع $g(\cdot)$   یک مقدار عددی\زیرنویس{Scalar} بدون واحد می‌باشد و همچنین برای اینکه خروجی انتگرال فازی خط \ref{alg:fci_combinator:line:fci} را بتوان به عنوان مقادیر جدول $Q$ مشارکتی جدید در نظر گرفت تا بتوانیم در خطوط \ref{alg:proposed:QCO_replacement} الگوریتم \ref{alg:proposed} به عنوان جدول $Q$ تک‌تک عامل‌ها جایگذاری کنیم باید خروجی انتگرال فازی خط \ref{alg:fci_combinator:line:fci} الگوریتم \ref{alg:fci_combinator} از جنس جدول‌های $Q$ عامل‌ها باشد در نتیجه تابع $f(\cdot)$ باید یک تابع خطی بصورت \ref{eq:fci_f_func} باشد تا خروجی انتگرال فازی همجنس مقادیر $\vec{x}$ باشد.
\begin{equation}\label{eq:fci_f_func}
f(\omega) = a\omega + b
\end{equation}
متغییر‌های $a$ و $b$ در \ref{eq:fci_f_func} می‌تواند به عنوان پارامترهای سازگار\زیرنویس{\مق{Addaptive Parameters}} در میزان کیفیت جدول $Q$ مشارکتی خروجی الگوریتم \ref{alg:fci_combinator} موثر واقع شود ولی با این‌حال در این پژوهش مقادیر $a$ و $b$ هردو به ترتیب مقادیر ثابت ۱ و صفر در نظر گرفته شده‌اند که یعنی از تابع همانی به عنوان تابع $f(\cdot)$ استفاده شده است.

تابع $g(\cdot)$ یک ورودی مرتب شده طبق آنچه که در \ref{eq:fi_choquet_perm_val} آمده است می‌گیرد و در الگوریتم \ref{alg:fci_combinator} تعیین این تابع تاثیر زیادی بروی کیفیت خروجی الگوریتم خواهد داشت ولی چالش‌هایی برای تعیین این تابع داریم؛ تابع $g(\cdot)$ باید دارای ویژگی‌های زیر باشد:
\begin{enumerate}
\فقره \textbf{پویا\زیرنویس{Dynamic} باشد:} از آنجایی که تابع $g(\cdot)$ میزان اندازه‌گیری غیرافزایشی\زیرنویس{Non-additive} منابع اطلاعاتی را در اختیار می‌گذارد\مرجع{torra2014non}، نیاز داریم تعیین کنیم که کدام منابع اطلاعاتی(در اینجا خبرگی عامل‌ها) در کنار هم چه ارزش افزوده‌ای دارد؛ ولی از آنجایی که در حین یادگیری مشترک روشی برای تعیین این ارزش افزوده نداریم بنابراین باید تابع $g(\cdot)$ بصورت پویا بتواند مقادیر این ارزش افزوده را تخمین بزند.
\فقره \textbf{قابل گسترش \زیرنویس{Expandable} باشد:} زیرا که تعداد عامل‌ها در محیط متغیر است لذا باید تابع $g(\cdot)$ بگونه‌ای باشد به ازای تغییر تعداد عامل‌ها (که تغییر در تعداد اعضای بردار $\vec{A}$ را در پی دارد) قابل گسترش باشد.
\end{enumerate}
 یکی از روش‌ تخمین $g(\cdot)$ که دو ویژگی بالا را داشته باشد، تابع اندازه‌گیری-$\lambda$ سوگنو می‌باشد ولی این تابع نیاز به ریشه‌یابی روی متغییر $\lambda$ دارد که طبق آنچه که در \ref{eq:sugeno-lambda-measure:rooting} آمده است به ازای تعداد عامل‌های مختلف نیاز به ریشه‌یابی معادلات غیرخطی دارد که بدلیل پیچدگی محاسباتی این ریشه‌‌یابی و همچنین طبق نتایج حاصل از دستاوردهای این پژوهش که در فصل نتیجه‌گیری آورده شده است، در آزمایشات صورت گرفته در این پژوهش از تابع اندازه‌گیری-$\lambda$ سوگنو به عنوان تابع $g(\cdot)$ استفاده نشده است. یک سری توابع در این پژوهش بجهت استفاده، آزمایش و نتیجه‌گیری به عنوان $g(\cdot)$ معرفی شده است که این توابع در الگوریتم‌های \ref{alg:g:const-one} تا \ref{alg:g:k-mean} آمده‌اند.

\begin{algorithm}[t]\setstretch{1.2}
\caption{الگوریتم \مق{Const-One} برای تخمین تابع $g(\cdot)$ در الگوریتم \ref{alg:fci_combinator}}\label{alg:g:const-one}
\begin{latin}
\begin{algorithmic}[1]
\Procedure{Const-One}{$\vec{A}_i$}
	\If {\Call{length}{$\vec{A}_i$} $\geq m$}
		\State \Return 1;
	\ElsIf {\Call{length}{$\vec{A}_i$} = 0}
		\State \Return 0;
	\Else
		\State \Return $1$;
	\EndIf
\EndProcedure
\end{algorithmic}
\end{latin}
\end{algorithm}

\begin{algorithm}[t]\setstretch{1.2}
\caption{الگوریتم \مق{Max} برای تخمین تابع $g(\cdot)$ در الگوریتم \ref{alg:fci_combinator}}\label{alg:g:max}
\begin{latin}
\begin{algorithmic}[1]
\Procedure{Max}{$\vec{A}_i$}
	\If {\Call{length}{$\vec{A}_i$} $\geq m$}
		\State \Return 1;
	\ElsIf {\Call{length}{$\vec{A}_i$} = 0}
		\State \Return 0;
	\Else
		\State \Return $\max\limits_{\vec{A}_i}$;
	\EndIf
\EndProcedure
\end{algorithmic}
\end{latin}
\end{algorithm}

\begin{algorithm}[t]\setstretch{1.2}
\caption{الگوریتم \مق{Mean} برای تخمین تابع $g(\cdot)$ در الگوریتم \ref{alg:fci_combinator}}\label{alg:g:mean}
\begin{latin}
\begin{algorithmic}[1]
\Procedure{Mean}{$\vec{A}_i$}
	\If {\Call{length}{$\vec{A}_i$} $\geq m$}
		\State \Return 1;
	\ElsIf {\Call{length}{$\vec{A}_i$} = 0}
		\State \Return 0;
	\Else
		\State \Return ${\sum\limits_{j=i}^{\Call{\text{length}}{\vec{B}_i} + i} \vec{A}_i(j) \over \Call{\text{length}}{\vec{A}_i}}$;
	\EndIf
\EndProcedure
\end{algorithmic}
\end{latin}
\end{algorithm}

\begin{algorithm}[t]\setstretch{1.2}
\caption{الگوریتم \مق{K-Mean} برای تخمین تابع $g(\cdot)$ در الگوریتم \ref{alg:fci_combinator}}\label{alg:g:k-mean}
\begin{latin}
\begin{algorithmic}[1]
\Procedure{K-Mean}{$\vec{A}_i$}
	\If {\Call{length}{$\vec{A}_i$} $\geq m$}
		\State \Return 1;
	\ElsIf {\Call{length}{$\vec{A}_i$} = 0}
		\State \Return 0;
	\Else
		\State $\vec{B}_i = \Call{Sort-Ascending}{\vec{A}_i}$;
		\State \Return ${\sum\limits_{j=i, k = 1}^{\Call{\text{length}}{\vec{B}_i} + i, \Call{\text{length}}{\vec{B}_i}}  k \cdot \vec{B}_i(j) \over {(\sum\limits_{j=1}^{\Call{\text{length}}{\vec{B}_i}} j) - 1}}$;
	\EndIf
\EndProcedure
\end{algorithmic}
\end{latin}
\end{algorithm}

در الگوریتم \ref{alg:g:const-one} به ازای هر ورودی دلخواد مقدار ثابت ۱ به عنوان خروجی برگشت داده می‌شود، این بدین معنی است که ارزش افزوده‌ی هرنوع ترکیب اطلاعاتی(خبرگی) برای ما دارای حداکثر ارزش می‌باشد و این مساله باعث می‌شود که نتیجه‌ی انتگرال فازی خط \ref{alg:fci_combinator:line:fci} الگوریتم \ref{alg:fci_combinator} مقداری معادل با مقدار خبره‌ترین عامل(عاملی که کمترین پرسه را در محیط مربوطه داشته) را به عنوان مقدار جدید جدول $Q$ مشارکتی تولید کند.

در الگوریتم \ref{alg:g:max} میزان خبرگی خبره‌ترین عامل به عنوان خروجی تابع $g(\cdot)$ برگشت داده می‌شود. در الگوریتم \ref{alg:g:mean} خروجی، میانگین خبرگی عامل‌ها در نظر گرفته شده است و در الگوریتم \ref{alg:g:k-mean} طبق رابطه‌ی نوشته شده میانگین $k$ام میزان خبرگی‌ها به عنوان خروجی برمی‌گردد به طوری که بزرگترین خبرگی در عدد $k$ و کوچکترین خبرگی در عدد ۱ و هر آنچه که مابین این دو خبرگی وجود دارد در اندیس ترتیب مرتب شده آن‌ها ضرب می‌شود و میانگین این مجموع محاسبه می‌شود و برگشت داده می‌شود.